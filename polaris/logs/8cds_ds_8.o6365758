+ export 'CXXFLAGS= -fpermissive'
+ CXXFLAGS=' -fpermissive'
+ ENV_NAME_PATH=/lus/eagle/projects/SR-APPFL/duo/env/sft
+ APP_DIR=/lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris
+ SCRIPT_PATH=/lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/sft.py
+ DS_CFG=/lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/deepspeed_zero3.yaml
+ export TRITON_DISABLE_TMA=1
+ TRITON_DISABLE_TMA=1
+ export 'CXXFLAGS= -fpermissive -fpermissive'
+ CXXFLAGS=' -fpermissive -fpermissive'
+ export DS_BUILD_SPARSE_ATTN=0
+ DS_BUILD_SPARSE_ATTN=0
+ export DS_SKIP_CUDA_BUILD=1
+ DS_SKIP_CUDA_BUILD=1
+ export GPUS_PER_NODE=4
+ GPUS_PER_NODE=4
++ wc -l
+ export NNODES=2
+ NNODES=2
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
++ head -n1 /var/spool/pbs/aux/6365758.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov
+ HEADNODE=x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov
+ HEADIP=x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov
+ export RDZV_PORT=12355
+ RDZV_PORT=12355
+ export OMP_NUM_THREADS=8
+ OMP_NUM_THREADS=8
+ export TMPDIR=/tmp/zhangduo4610_6365758
+ TMPDIR=/tmp/zhangduo4610_6365758
+ mkdir -p /tmp/zhangduo4610_6365758
+ export TRITON_CACHE_DIR_BASE=/tmp/zhangduo4610_6365758/triton
+ TRITON_CACHE_DIR_BASE=/tmp/zhangduo4610_6365758/triton
+ export TORCH_EXTENSIONS_DIR_BASE=/tmp/zhangduo4610_6365758/torch_ext
+ TORCH_EXTENSIONS_DIR_BASE=/tmp/zhangduo4610_6365758/torch_ext
+ mkdir -p /tmp/zhangduo4610_6365758/triton /tmp/zhangduo4610_6365758/torch_ext
+ PY=/lus/eagle/projects/SR-APPFL/duo/env/sft/bin/python
+ export PYTHONNOUSERSITE=1
+ PYTHONNOUSERSITE=1
+ LAUNCHER='
  export RANK=${PMI_RANK:-0} ;
  export LOCAL_RANK=${PMI_LOCAL_RANK:-$((RANK % 4))} ;
  export TRITON_CACHE_DIR=/tmp/zhangduo4610_6365758/triton/$RANK ;
  export TORCH_EXTENSIONS_DIR=/tmp/zhangduo4610_6365758/torch_ext/$RANK ;
  mkdir -p "$TRITON_CACHE_DIR" "$TORCH_EXTENSIONS_DIR" ;

# --- Polaris modules ---
  module use /soft/modulefiles
  module load gcc-native/12.3
  module load conda/2024-04-29-aws-nccl
  module load cudatoolkit-standalone/12.4.0
  unset http_proxy https_proxy HTTP_PROXY
  conda activate /lus/eagle/projects/SR-APPFL/duo/env/sft

  HF_HUB_ENABLE_HF_TRANSFER=1 \
  ACCELERATE_LOG_LEVEL=info \
  TRANSFORMERS_VERBOSITY=info \
  python -m accelerate.commands.launch \
    --config_file /lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/deepspeed_zero3.yaml \
    --num_machines 2 \
    --num_processes 8 \
    --main_process_ip x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov \
    --main_process_port 12355 \
    --machine_rank ${PMI_RANK:-0} \
    --tee 3 \
    /lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/sft.py
'
+ mpiexec -n 2 -ppn 1 bash -lc '
  export RANK=${PMI_RANK:-0} ;
  export LOCAL_RANK=${PMI_LOCAL_RANK:-$((RANK % 4))} ;
  export TRITON_CACHE_DIR=/tmp/zhangduo4610_6365758/triton/$RANK ;
  export TORCH_EXTENSIONS_DIR=/tmp/zhangduo4610_6365758/torch_ext/$RANK ;
  mkdir -p "$TRITON_CACHE_DIR" "$TORCH_EXTENSIONS_DIR" ;

# --- Polaris modules ---
  module use /soft/modulefiles
  module load gcc-native/12.3
  module load conda/2024-04-29-aws-nccl
  module load cudatoolkit-standalone/12.4.0
  unset http_proxy https_proxy HTTP_PROXY
  conda activate /lus/eagle/projects/SR-APPFL/duo/env/sft

  HF_HUB_ENABLE_HF_TRANSFER=1 \
  ACCELERATE_LOG_LEVEL=info \
  TRANSFORMERS_VERBOSITY=info \
  python -m accelerate.commands.launch \
    --config_file /lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/deepspeed_zero3.yaml \
    --num_machines 2 \
    --num_processes 8 \
    --main_process_ip x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov \
    --main_process_port 12355 \
    --machine_rank ${PMI_RANK:-0} \
    --tee 3 \
    /lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/sft.py
'

The following have been reloaded with a version change:
  1) conda/2024-04-29 => conda/2024-04-29-aws-nccl


The following have been reloaded with a version change:
  1) conda/2024-04-29 => conda/2024-04-29-aws-nccl

[2025-10-02 21:51:47,324] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-02 21:51:47,324] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-02 21:52:57,760] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-02 21:52:57,760] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default0]:PyTorch: setting up devices
[default3]:PyTorch: setting up devices
[default1]:PyTorch: setting up devices
[default1]:PyTorch: setting up devices
[default3]:PyTorch: setting up devices
[default2]:PyTorch: setting up devices
[default2]:PyTorch: setting up devices
[default0]:PyTorch: setting up devices
[default0]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default1]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default3]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default2]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default2]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default3]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default1]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default0]:The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[default2]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default2]:Model config LlamaConfig {
[default2]:  "architectures": [
[default2]:    "LlamaForCausalLM"
[default2]:  ],
[default2]:  "attention_bias": false,
[default2]:  "attention_dropout": 0.0,
[default2]:  "bos_token_id": 128000,
[default2]:  "eos_token_id": 128009,
[default2]:  "head_dim": 128,
[default2]:  "hidden_act": "silu",
[default2]:  "hidden_size": 4096,
[default2]:  "initializer_range": 0.02,
[default2]:  "intermediate_size": 14336,
[default2]:  "max_position_embeddings": 8192,
[default2]:  "mlp_bias": false,
[default2]:  "model_type": "llama",
[default2]:  "num_attention_heads": 32,
[default2]:  "num_hidden_layers": 32,
[default2]:  "num_key_value_heads": 8,
[default2]:  "pretraining_tp": 1,
[default2]:  "rms_norm_eps": 1e-05,
[default2]:  "rope_scaling": null,
[default2]:  "rope_theta": 500000.0,
[default2]:  "tie_word_embeddings": false,
[default2]:  "torch_dtype": "bfloat16",
[default2]:  "transformers_version": "4.55.0",
[default2]:  "use_cache": true,
[default2]:  "vocab_size": 128256
[default2]:}
[default2]:
[default3]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default3]:Model config LlamaConfig {
[default3]:  "architectures": [
[default3]:    "LlamaForCausalLM"
[default3]:  ],
[default3]:  "attention_bias": false,
[default3]:  "attention_dropout": 0.0,
[default3]:  "bos_token_id": 128000,
[default3]:  "eos_token_id": 128009,
[default3]:  "head_dim": 128,
[default3]:  "hidden_act": "silu",
[default3]:  "hidden_size": 4096,
[default3]:  "initializer_range": 0.02,
[default3]:  "intermediate_size": 14336,
[default3]:  "max_position_embeddings": 8192,
[default3]:  "mlp_bias": false,
[default3]:  "model_type": "llama",
[default3]:  "num_attention_heads": 32,
[default3]:  "num_hidden_layers": 32,
[default3]:  "num_key_value_heads": 8,
[default3]:  "pretraining_tp": 1,
[default3]:  "rms_norm_eps": 1e-05,
[default3]:  "rope_scaling": null,
[default3]:  "rope_theta": 500000.0,
[default3]:  "tie_word_embeddings": false,
[default3]:  "torch_dtype": "bfloat16",
[default3]:  "transformers_version": "4.55.0",
[default3]:  "use_cache": true,
[default3]:  "vocab_size": 128256
[default3]:}
[default3]:
[default1]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default1]:Model config LlamaConfig {
[default1]:  "architectures": [
[default1]:    "LlamaForCausalLM"
[default1]:  ],
[default1]:  "attention_bias": false,
[default1]:  "attention_dropout": 0.0,
[default1]:  "bos_token_id": 128000,
[default1]:  "eos_token_id": 128009,
[default1]:  "head_dim": 128,
[default1]:  "hidden_act": "silu",
[default1]:  "hidden_size": 4096,
[default1]:  "initializer_range": 0.02,
[default1]:  "intermediate_size": 14336,
[default1]:  "max_position_embeddings": 8192,
[default1]:  "mlp_bias": false,
[default1]:  "model_type": "llama",
[default1]:  "num_attention_heads": 32,
[default1]:  "num_hidden_layers": 32,
[default1]:  "num_key_value_heads": 8,
[default1]:  "pretraining_tp": 1,
[default1]:  "rms_norm_eps": 1e-05,
[default1]:  "rope_scaling": null,
[default1]:  "rope_theta": 500000.0,
[default1]:  "tie_word_embeddings": false,
[default1]:  "torch_dtype": "bfloat16",
[default1]:  "transformers_version": "4.55.0",
[default1]:  "use_cache": true,
[default1]:  "vocab_size": 128256
[default1]:}
[default1]:
[default0]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default0]:Model config LlamaConfig {
[default0]:  "architectures": [
[default0]:    "LlamaForCausalLM"
[default0]:  ],
[default0]:  "attention_bias": false,
[default0]:  "attention_dropout": 0.0,
[default0]:  "bos_token_id": 128000,
[default0]:  "eos_token_id": 128009,
[default0]:  "head_dim": 128,
[default0]:  "hidden_act": "silu",
[default0]:  "hidden_size": 4096,
[default0]:  "initializer_range": 0.02,
[default0]:  "intermediate_size": 14336,
[default0]:  "max_position_embeddings": 8192,
[default0]:  "mlp_bias": false,
[default0]:  "model_type": "llama",
[default0]:  "num_attention_heads": 32,
[default0]:  "num_hidden_layers": 32,
[default0]:  "num_key_value_heads": 8,
[default0]:  "pretraining_tp": 1,
[default0]:  "rms_norm_eps": 1e-05,
[default0]:  "rope_scaling": null,
[default0]:  "rope_theta": 500000.0,
[default0]:  "tie_word_embeddings": false,
[default0]:  "torch_dtype": "bfloat16",
[default0]:  "transformers_version": "4.55.0",
[default0]:  "use_cache": true,
[default0]:  "vocab_size": 128256
[default0]:}
[default0]:
[default3]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default3]:Model config LlamaConfig {
[default3]:  "architectures": [
[default3]:    "LlamaForCausalLM"
[default3]:  ],
[default3]:  "attention_bias": false,
[default3]:  "attention_dropout": 0.0,
[default3]:  "bos_token_id": 128000,
[default3]:  "eos_token_id": 128009,
[default3]:  "head_dim": 128,
[default3]:  "hidden_act": "silu",
[default3]:  "hidden_size": 4096,
[default3]:  "initializer_range": 0.02,
[default3]:  "intermediate_size": 14336,
[default3]:  "max_position_embeddings": 8192,
[default3]:  "mlp_bias": false,
[default3]:  "model_type": "llama",
[default3]:  "num_attention_heads": 32,
[default3]:  "num_hidden_layers": 32,
[default3]:  "num_key_value_heads": 8,
[default3]:  "pretraining_tp": 1,
[default3]:  "rms_norm_eps": 1e-05,
[default3]:  "rope_scaling": null,
[default3]:  "rope_theta": 500000.0,
[default3]:  "tie_word_embeddings": false,
[default3]:  "torch_dtype": "bfloat16",
[default3]:  "transformers_version": "4.55.0",
[default3]:  "use_cache": true,
[default3]:  "vocab_size": 128256
[default3]:}
[default3]:
[default1]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default1]:Model config LlamaConfig {
[default1]:  "architectures": [
[default1]:    "LlamaForCausalLM"
[default1]:  ],
[default1]:  "attention_bias": false,
[default1]:  "attention_dropout": 0.0,
[default1]:  "bos_token_id": 128000,
[default1]:  "eos_token_id": 128009,
[default1]:  "head_dim": 128,
[default1]:  "hidden_act": "silu",
[default1]:  "hidden_size": 4096,
[default1]:  "initializer_range": 0.02,
[default1]:  "intermediate_size": 14336,
[default1]:  "max_position_embeddings": 8192,
[default1]:  "mlp_bias": false,
[default1]:  "model_type": "llama",
[default1]:  "num_attention_heads": 32,
[default1]:  "num_hidden_layers": 32,
[default1]:  "num_key_value_heads": 8,
[default1]:  "pretraining_tp": 1,
[default1]:  "rms_norm_eps": 1e-05,
[default1]:  "rope_scaling": null,
[default1]:  "rope_theta": 500000.0,
[default1]:  "tie_word_embeddings": false,
[default1]:  "torch_dtype": "bfloat16",
[default1]:  "transformers_version": "4.55.0",
[default1]:  "use_cache": true,
[default1]:  "vocab_size": 128256
[default1]:}
[default1]:
[default0]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default0]:Model config LlamaConfig {
[default0]:  "architectures": [
[default0]:    "LlamaForCausalLM"
[default0]:  ],
[default0]:  "attention_bias": false,
[default0]:  "attention_dropout": 0.0,
[default0]:  "bos_token_id": 128000,
[default0]:  "eos_token_id": 128009,
[default0]:  "head_dim": 128,
[default0]:  "hidden_act": "silu",
[default0]:  "hidden_size": 4096,
[default0]:  "initializer_range": 0.02,
[default0]:  "intermediate_size": 14336,
[default0]:  "max_position_embeddings": 8192,
[default0]:  "mlp_bias": false,
[default0]:  "model_type": "llama",
[default0]:  "num_attention_heads": 32,
[default0]:  "num_hidden_layers": 32,
[default0]:  "num_key_value_heads": 8,
[default0]:  "pretraining_tp": 1,
[default0]:  "rms_norm_eps": 1e-05,
[default0]:  "rope_scaling": null,
[default0]:  "rope_theta": 500000.0,
[default0]:  "tie_word_embeddings": false,
[default0]:  "torch_dtype": "bfloat16",
[default0]:  "transformers_version": "4.55.0",
[default0]:  "use_cache": true,
[default0]:  "vocab_size": 128256
[default0]:}
[default0]:
[default2]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/config.json
[default2]:Model config LlamaConfig {
[default2]:  "architectures": [
[default2]:    "LlamaForCausalLM"
[default2]:  ],
[default2]:  "attention_bias": false,
[default2]:  "attention_dropout": 0.0,
[default2]:  "bos_token_id": 128000,
[default2]:  "eos_token_id": 128009,
[default2]:  "head_dim": 128,
[default2]:  "hidden_act": "silu",
[default2]:  "hidden_size": 4096,
[default2]:  "initializer_range": 0.02,
[default2]:  "intermediate_size": 14336,
[default2]:  "max_position_embeddings": 8192,
[default2]:  "mlp_bias": false,
[default2]:  "model_type": "llama",
[default2]:  "num_attention_heads": 32,
[default2]:  "num_hidden_layers": 32,
[default2]:  "num_key_value_heads": 8,
[default2]:  "pretraining_tp": 1,
[default2]:  "rms_norm_eps": 1e-05,
[default2]:  "rope_scaling": null,
[default2]:  "rope_theta": 500000.0,
[default2]:  "tie_word_embeddings": false,
[default2]:  "torch_dtype": "bfloat16",
[default2]:  "transformers_version": "4.55.0",
[default2]:  "use_cache": true,
[default2]:  "vocab_size": 128256
[default2]:}
[default2]:
[default2]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default2]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default2]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default3]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default3]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default3]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default1]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default1]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default1]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default0]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default0]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default0]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default3]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default3]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default3]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default1]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default1]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default1]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default0]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default0]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default0]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default2]:loading weights file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/model.safetensors.index.json
[default2]:Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[default2]:Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[default2]:Generate config GenerationConfig {
[default2]:  "bos_token_id": 128000,
[default2]:  "eos_token_id": 128009
[default2]:}
[default2]:
[default3]:Generate config GenerationConfig {
[default3]:  "bos_token_id": 128000,
[default3]:  "eos_token_id": 128009
[default3]:}
[default3]:
[default1]:Generate config GenerationConfig {
[default1]:  "bos_token_id": 128000,
[default1]:  "eos_token_id": 128009
[default1]:}
[default1]:
[default0]:Generate config GenerationConfig {
[default0]:  "bos_token_id": 128000,
[default0]:  "eos_token_id": 128009
[default0]:}
[default0]:
[default3]:Generate config GenerationConfig {
[default3]:  "bos_token_id": 128000,
[default3]:  "eos_token_id": 128009
[default3]:}
[default3]:
[default1]:Generate config GenerationConfig {
[default1]:  "bos_token_id": 128000,
[default1]:  "eos_token_id": 128009
[default1]:}
[default1]:
[default0]:Generate config GenerationConfig {
[default0]:  "bos_token_id": 128000,
[default0]:  "eos_token_id": 128009
[default0]:}
[default0]:
[default2]:Generate config GenerationConfig {
[default2]:  "bos_token_id": 128000,
[default2]:  "eos_token_id": 128009
[default2]:}
[default2]:
[default1]:
[default2]:
[default2]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default3]:
[default1]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default2]:
[default2]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default0]:
[default0]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default3]:
[default3]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default1]:
[default1]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default0]:
[default3]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default3]:
[default3]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it][default1]:
[default0]:Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][default1]:
[default1]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it][default0]:
[default0]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it][default2]:
[default2]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.54s/it][default3]:
[default1]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it][default2]:
[default2]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.53s/it][default0]:
[default3]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it][default1]:
[default1]:Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:44, 22.11s/it][default0]:
[default0]:Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:44, 22.11s/it][default2]:
[default0]:Loading checkpoint shards:  25%|██▌       | 1/4 [00:33<01:41, 33.72s/it][default3]:
[default3]:Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:44, 22.11s/it][default1]:
[default1]:Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:44, 22.11s/it][default2]:
[default2]:Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:44, 22.11s/it][default3]:
[default2]:Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:44, 22.11s/it][default0]:
[default3]:Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:44, 22.12s/it][default1]:
[default1]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:28, 28.16s/it][default0]:
[default0]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:28, 28.16s/it][default2]:
[default2]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:28, 28.16s/it][default3]:
[default0]:Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:10, 35.35s/it][default3]:
[default3]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:28, 28.16s/it][default1]:
[default1]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:28, 28.16s/it][default2]:
[default3]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:28, 28.16s/it][default1]:
[default1]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 17.78s/it]
[default1]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.82s/it]
[default1]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default1]:
[default1]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default1]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default0]:
[default0]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 17.78s/it]
[default0]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.82s/it]
[default0]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default0]:
[default0]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default0]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default2]:
[default2]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 17.78s/it]
[default2]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.82s/it]
[default2]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default2]:
[default2]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default2]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default3]:
[default3]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 17.78s/it]
[default3]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.82s/it]
[default3]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default3]:
[default3]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default3]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default2]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:28, 28.16s/it][default3]:
[default3]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 17.78s/it]
[default3]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.82s/it]
[default3]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default3]:
[default3]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default3]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default3]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default3]:Generate config GenerationConfig {
[default3]:  "bos_token_id": 128000,
[default3]:  "do_sample": true,
[default3]:  "eos_token_id": [
[default3]:    128001,
[default3]:    128009
[default3]:  ],
[default3]:  "max_length": 4096,
[default3]:  "temperature": 0.6,
[default3]:  "top_p": 0.9
[default3]:}
[default3]:
[default1]:
[default1]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 17.78s/it]
[default1]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.82s/it]
[default1]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default1]:
[default1]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default1]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default1]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default1]:Generate config GenerationConfig {
[default1]:  "bos_token_id": 128000,
[default1]:  "do_sample": true,
[default1]:  "eos_token_id": [
[default1]:    128001,
[default1]:    128009
[default1]:  ],
[default1]:  "max_length": 4096,
[default1]:  "temperature": 0.6,
[default1]:  "top_p": 0.9
[default1]:}
[default1]:
[default2]:
[default2]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 17.78s/it]
[default2]:Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.82s/it]
[default2]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default2]:
[default2]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default2]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default2]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default2]:Generate config GenerationConfig {
[default2]:  "bos_token_id": 128000,
[default2]:  "do_sample": true,
[default2]:  "eos_token_id": [
[default2]:    128001,
[default2]:    128009
[default2]:  ],
[default2]:  "max_length": 4096,
[default2]:  "temperature": 0.6,
[default2]:  "top_p": 0.9
[default2]:}
[default2]:
[default1]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default1]:Generate config GenerationConfig {
[default1]:  "bos_token_id": 128000,
[default1]:  "do_sample": true,
[default1]:  "eos_token_id": [
[default1]:    128001,
[default1]:    128009
[default1]:  ],
[default1]:  "max_length": 4096,
[default1]:  "temperature": 0.6,
[default1]:  "top_p": 0.9
[default1]:}
[default1]:
[default1]:loading file tokenizer.json
[default1]:loading file tokenizer.model
[default1]:loading file added_tokens.json
[default1]:loading file special_tokens_map.json
[default1]:loading file tokenizer_config.json
[default1]:loading file chat_template.jinja
[default0]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default0]:Generate config GenerationConfig {
[default0]:  "bos_token_id": 128000,
[default0]:  "do_sample": true,
[default0]:  "eos_token_id": [
[default0]:    128001,
[default0]:    128009
[default0]:  ],
[default0]:  "max_length": 4096,
[default0]:  "temperature": 0.6,
[default0]:  "top_p": 0.9
[default0]:}
[default0]:
[default0]:loading file tokenizer.json
[default0]:loading file tokenizer.model
[default0]:loading file added_tokens.json
[default0]:loading file special_tokens_map.json
[default0]:loading file tokenizer_config.json
[default0]:loading file chat_template.jinja
[default2]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default2]:Generate config GenerationConfig {
[default2]:  "bos_token_id": 128000,
[default2]:  "do_sample": true,
[default2]:  "eos_token_id": [
[default2]:    128001,
[default2]:    128009
[default2]:  ],
[default2]:  "max_length": 4096,
[default2]:  "temperature": 0.6,
[default2]:  "top_p": 0.9
[default2]:}
[default2]:
[default2]:loading file tokenizer.json
[default2]:loading file tokenizer.model
[default2]:loading file added_tokens.json
[default2]:loading file special_tokens_map.json
[default2]:loading file tokenizer_config.json
[default2]:loading file chat_template.jinja
[default3]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default3]:Generate config GenerationConfig {
[default3]:  "bos_token_id": 128000,
[default3]:  "do_sample": true,
[default3]:  "eos_token_id": [
[default3]:    128001,
[default3]:    128009
[default3]:  ],
[default3]:  "max_length": 4096,
[default3]:  "temperature": 0.6,
[default3]:  "top_p": 0.9
[default3]:}
[default3]:
[default3]:loading file tokenizer.json
[default3]:loading file tokenizer.model
[default3]:loading file added_tokens.json
[default3]:loading file special_tokens_map.json
[default3]:loading file tokenizer_config.json
[default3]:loading file chat_template.jinja
[default3]:loading file tokenizer.json
[default3]:loading file tokenizer.model
[default3]:loading file added_tokens.json
[default3]:loading file special_tokens_map.json
[default3]:loading file tokenizer_config.json
[default3]:loading file chat_template.jinja
[default1]:loading file tokenizer.json
[default1]:loading file tokenizer.model
[default1]:loading file added_tokens.json
[default1]:loading file special_tokens_map.json
[default1]:loading file tokenizer_config.json
[default1]:loading file chat_template.jinja
[default2]:loading file tokenizer.json
[default2]:loading file tokenizer.model
[default2]:loading file added_tokens.json
[default2]:loading file special_tokens_map.json
[default2]:loading file tokenizer_config.json
[default2]:loading file chat_template.jinja
[default3]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default3]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default3]:  warnings.warn(
[default1]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default1]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default1]:  warnings.warn(
[default2]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default2]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default2]:  warnings.warn(
[default1]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default1]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default1]:  warnings.warn(
[default0]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default0]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default0]:  warnings.warn(
[default2]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default2]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default2]:  warnings.warn(
[default3]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default3]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default3]:  warnings.warn(
[default1]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default1]:  warnings.warn(
[default0]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default0]:  warnings.warn(
[default2]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default2]:  warnings.warn(
[default3]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default3]:  warnings.warn(
[default3]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default3]:  warnings.warn(
[default1]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default1]:  warnings.warn(
[default2]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default2]:  warnings.warn(
[default3]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default3]:  warnings.warn(
[default1]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default1]:  warnings.warn(
[default2]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default2]:  warnings.warn(
[default1]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default1]:  warnings.warn(
[default0]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default0]:  warnings.warn(
[default2]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default2]:  warnings.warn(
[default3]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default3]:  warnings.warn(
[default0]:
[default0]:Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:35, 35.34s/it][default0]:
[default0]:Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 23.94s/it]
[default0]:Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.00s/it]
[default0]:All model checkpoint weights were used when initializing LlamaForCausalLM.
[default0]:
[default0]:All the weights of LlamaForCausalLM were initialized from the model checkpoint at /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct.
[default0]:If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[default0]:loading configuration file /lus/eagle/projects/SR-APPFL/duo/models/llama3-8b-instruct/generation_config.json
[default0]:Generate config GenerationConfig {
[default0]:  "bos_token_id": 128000,
[default0]:  "do_sample": true,
[default0]:  "eos_token_id": [
[default0]:    128001,
[default0]:    128009
[default0]:  ],
[default0]:  "max_length": 4096,
[default0]:  "temperature": 0.6,
[default0]:  "top_p": 0.9
[default0]:}
[default0]:
[default0]:loading file tokenizer.json
[default0]:loading file tokenizer.model
[default0]:loading file added_tokens.json
[default0]:loading file special_tokens_map.json
[default0]:loading file tokenizer_config.json
[default0]:loading file chat_template.jinja
[default0]:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[default0]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: /home/zhangduo4610/CodeAlpaca-20k.
[default0]:  warnings.warn(
[default0]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.
[default0]:  warnings.warn(
[default0]:/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.
[default0]:  warnings.warn(
[default0]:
[default0]:Adding EOS to train dataset (num_proc=64):   0%|          | 0/10011 [00:00<?, ? examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):   1%|          | 92/10011 [00:00<01:44, 94.83 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):   3%|▎         | 283/10011 [00:01<00:38, 251.70 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):   4%|▍         | 439/10011 [00:01<00:24, 388.85 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):   6%|▋         | 628/10011 [00:02<00:29, 312.81 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  16%|█▌        | 1570/10011 [00:02<00:07, 1169.41 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  20%|██        | 2041/10011 [00:02<00:05, 1491.61 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  25%|██▌       | 2512/10011 [00:02<00:04, 1543.48 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  28%|██▊       | 2826/10011 [00:03<00:05, 1304.10 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  38%|███▊      | 3768/10011 [00:03<00:02, 2313.79 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  42%|████▏     | 4238/10011 [00:03<00:02, 2647.46 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  52%|█████▏    | 5175/10011 [00:03<00:01, 3752.94 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  61%|██████    | 6111/10011 [00:03<00:00, 4750.17 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  69%|██████▉   | 6891/10011 [00:03<00:00, 5149.86 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  77%|███████▋  | 7669/10011 [00:03<00:00, 4420.93 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  83%|████████▎ | 8295/10011 [00:04<00:00, 4728.21 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  91%|█████████ | 9075/10011 [00:04<00:00, 5188.47 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64):  98%|█████████▊| 9855/10011 [00:04<00:00, 5664.27 examples/s][default0]:
[default0]:Adding EOS to train dataset (num_proc=64): 100%|██████████| 10011/10011 [00:04<00:00, 2050.27 examples/s]
[default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:00<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:04<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:05<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:06<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:07<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:08<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:10<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:11<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:12<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:13<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:14<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:15<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:17<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:18<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:19<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:20<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:21<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:22<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:23<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:25<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:26<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:27<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:28<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:29<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:31<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:32<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:33<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:34<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:35<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:36<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:38<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:39<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:40<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:41<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:42<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:43<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:45<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:46<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:47<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:49<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:50<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:51<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:53<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:54<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:55<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:56<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:57<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [00:58<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:00<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:01<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:02<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:03<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:04<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:05<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:07<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:08<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:09<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:10<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:11<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:12<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:13<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:14<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:15<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:16<?, ? examples/s][default0]:
[default0]:Tokenizing train dataset (num_proc=64):   0%|          | 0/10011 [01:16<?, ? examples/s]
[default0]:[rank0]: multiprocess.pool.RemoteTraceback: 
[default0]:[rank0]: """
[default0]:[rank0]: Traceback (most recent call last):
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/multiprocess/pool.py", line 125, in worker
[default0]:[rank0]:     result = (True, func(*args, **kwds))
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 678, in _write_generator_to_queue
[default0]:[rank0]:     for i, result in enumerate(func(**kwargs)):
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3428, in _map_single
[default0]:[rank0]:     example = apply_function_on_filtered_inputs(example, i, offset=offset)
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3320, in apply_function_on_filtered_inputs
[default0]:[rank0]:     processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 797, in tokenize
[default0]:[rank0]:     output = {"input_ids": processing_class(text=example[dataset_text_field])["input_ids"]}
[default0]:[rank0]: KeyError: 'text'
[default0]:[rank0]: """
[default0]:
[default0]:[rank0]: The above exception was the direct cause of the following exception:
[default0]:
[default0]:[rank0]: Traceback (most recent call last):
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/sft.py", line 182, in <module>
[default0]:[rank0]:     main()
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/sft.py", line 151, in main
[default0]:[rank0]:     trainer = SFTTrainer(
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 519, in __init__
[default0]:[rank0]:     train_dataset = self._prepare_dataset(
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 800, in _prepare_dataset
[default0]:[rank0]:     dataset = dataset.map(
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
[default0]:[rank0]:     out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 3147, in map
[default0]:[rank0]:     for rank, done, content in iflatmap_unordered(
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 718, in iflatmap_unordered
[default0]:[rank0]:     [async_result.get(timeout=0.05) for async_result in async_results]
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 718, in <listcomp>
[default0]:[rank0]:     [async_result.get(timeout=0.05) for async_result in async_results]
[default0]:[rank0]:   File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/multiprocess/pool.py", line 774, in get
[default0]:[rank0]:     raise self._value
[default0]:[rank0]: KeyError: 'text'
[default0]:[rank0]:[W1002 21:57:38.691598474 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1002 21:57:57.825000 416313 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 417540 closing signal SIGTERM
W1002 21:57:57.825000 416313 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 417541 closing signal SIGTERM
W1002 21:57:57.825000 416313 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 417542 closing signal SIGTERM
E1002 21:57:58.783000 416313 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 417539) of binary: /lus/eagle/projects/SR-APPFL/duo/env/sft/bin/python
Traceback (most recent call last):
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1245, in <module>
    main()
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1241, in main
    launch_command(args)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1220, in launch_command
    deepspeed_launcher(args)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 906, in deepspeed_launcher
    distrib_run.run(args)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/lus/eagle/projects/SR-APPFL/duo/LLM-trl/polaris/sft.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-02_21:57:57
  host      : x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 417539)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[default0]:[ds] Using CXX=/usr/bin/g++-12
[default1]:[ds] Using CXX=/usr/bin/g++-12
[default3]:[ds] Using CXX=/usr/bin/g++-12
[default2]:[ds] Using CXX=/usr/bin/g++-12
[default0]:[2025-10-02 21:53:51,072] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2025-10-02 21:53:51,073] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2025-10-02 21:53:51,071] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2025-10-02 21:53:51,071] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2025-10-02 21:53:57,874] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default0]:[2025-10-02 21:53:57,962] [INFO] [comm.py:821:init_distributed] cdb=None
[default0]:[2025-10-02 21:53:57,962] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default1]:[2025-10-02 21:53:57,875] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default1]:[2025-10-02 21:53:57,961] [INFO] [comm.py:821:init_distributed] cdb=None
[default3]:[2025-10-02 21:53:57,875] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default3]:[2025-10-02 21:53:57,963] [INFO] [comm.py:821:init_distributed] cdb=None
[default2]:[2025-10-02 21:53:57,876] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default2]:[2025-10-02 21:53:57,963] [INFO] [comm.py:821:init_distributed] cdb=None
[default2]:[2025-10-02 21:54:00,959] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[default3]:[2025-10-02 21:54:00,960] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[default1]:[2025-10-02 21:54:00,960] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[default0]:[2025-10-02 21:54:00,959] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[default0]:[2025-10-02 21:54:09,869] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
[default2]:[rank6]:[W1002 21:57:59.564802600 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=44, addr=[x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov]:43360, remote=[x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov]:12355): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[default2]:Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:682 (most recent call first):
[default2]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14ad6bd7eeb0 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libc10.so)
[default2]:frame #1: <unknown function> + 0x5d694d1 (0x14ad4fdbb4d1 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default2]:frame #2: <unknown function> + 0x5d6a8cd (0x14ad4fdbc8cd in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default2]:frame #3: <unknown function> + 0x5d6b47a (0x14ad4fdbd47a in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default2]:frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x31e (0x14ad4fdb819e in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default2]:frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x398 (0x14ad0f29db18 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
[default2]:frame #6: <unknown function> + 0xdbbf4 (0x14acf2c5abf4 in /lus/eagle/projects/SR-APPFL/duo/env/sft/bin/../lib/libstdc++.so.6)
[default2]:frame #7: <unknown function> + 0xa6f6c (0x14ad6ca8ef6c in /lib64/libc.so.6)
[default2]:frame #8: <unknown function> + 0x12e338 (0x14ad6cb16338 in /lib64/libc.so.6)
[default2]:
[default3]:[rank7]:[W1002 21:57:59.558966871 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=44, addr=[x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov]:43336, remote=[x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov]:12355): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[default3]:Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:682 (most recent call first):
[default3]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14ea2f2d9eb0 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libc10.so)
[default3]:frame #1: <unknown function> + 0x5d694d1 (0x14ea133bb4d1 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default3]:frame #2: <unknown function> + 0x5d6a8cd (0x14ea133bc8cd in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default3]:frame #3: <unknown function> + 0x5d6b47a (0x14ea133bd47a in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default3]:frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x31e (0x14ea133b819e in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default3]:frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x398 (0x14e9d289db18 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
[default3]:frame #6: <unknown function> + 0xdbbf4 (0x14e9b625abf4 in /lus/eagle/projects/SR-APPFL/duo/env/sft/bin/../lib/libstdc++.so.6)
[default3]:frame #7: <unknown function> + 0xa6f6c (0x14ea2ffc5f6c in /lib64/libc.so.6)
[default3]:frame #8: <unknown function> + 0x12e338 (0x14ea3004d338 in /lib64/libc.so.6)
[default3]:
[default1]:[rank5]:[W1002 21:57:59.561543890 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=44, addr=[x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov]:43352, remote=[x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov]:12355): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[default1]:Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:682 (most recent call first):
[default1]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14a31e6d9eb0 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libc10.so)
[default1]:frame #1: <unknown function> + 0x5d694d1 (0x14a3027bb4d1 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default1]:frame #2: <unknown function> + 0x5d6a8cd (0x14a3027bc8cd in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default1]:frame #3: <unknown function> + 0x5d6b47a (0x14a3027bd47a in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default1]:frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x31e (0x14a3027b819e in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default1]:frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x398 (0x14a2c1c9db18 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
[default1]:frame #6: <unknown function> + 0xdbbf4 (0x14a2a565abf4 in /lus/eagle/projects/SR-APPFL/duo/env/sft/bin/../lib/libstdc++.so.6)
[default1]:frame #7: <unknown function> + 0xa6f6c (0x14a31f3bef6c in /lib64/libc.so.6)
[default1]:frame #8: <unknown function> + 0x12e338 (0x14a31f446338 in /lib64/libc.so.6)
[default1]:
[default0]:[rank4]:[W1002 21:57:59.567303216 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=44, addr=[x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov]:35608, remote=[x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov]:12355): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[default0]:Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:682 (most recent call first):
[default0]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x1465a6d7eeb0 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libc10.so)
[default0]:frame #1: <unknown function> + 0x5d694d1 (0x14658adbb4d1 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default0]:frame #2: <unknown function> + 0x5d6a8cd (0x14658adbc8cd in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default0]:frame #3: <unknown function> + 0x5d6b47a (0x14658adbd47a in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default0]:frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x31e (0x14658adb819e in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
[default0]:frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x398 (0x14654a29db18 in /lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
[default0]:frame #6: <unknown function> + 0xdbbf4 (0x14652dc5abf4 in /lus/eagle/projects/SR-APPFL/duo/env/sft/bin/../lib/libstdc++.so.6)
[default0]:frame #7: <unknown function> + 0xa6f6c (0x1465a7aa9f6c in /lib64/libc.so.6)
[default0]:frame #8: <unknown function> + 0x12e338 (0x1465a7b31338 in /lib64/libc.so.6)
[default0]:
[default2]:[rank6]:[W1002 21:57:59.682868740 ProcessGroupNCCL.cpp:1783] [PG ID 0 PG GUID 0(default_pg) Rank 6] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[default3]:[rank7]:[W1002 21:57:59.682892174 ProcessGroupNCCL.cpp:1783] [PG ID 0 PG GUID 0(default_pg) Rank 7] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[default1]:[rank5]:[W1002 21:57:59.682934303 ProcessGroupNCCL.cpp:1783] [PG ID 0 PG GUID 0(default_pg) Rank 5] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[default0]:[rank4]:[W1002 21:57:59.682914125 ProcessGroupNCCL.cpp:1783] [PG ID 0 PG GUID 0(default_pg) Rank 4] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 0 exited with code 1
W1002 21:58:00.077000 3372986 site-packages/torch/distributed/elastic/agent/server/api.py:723] Received Signals.SIGTERM death signal, shutting down workers
W1002 21:58:00.078000 3372986 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3374214 closing signal SIGTERM
W1002 21:58:00.078000 3372986 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3374215 closing signal SIGTERM
W1002 21:58:00.078000 3372986 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3374216 closing signal SIGTERM
W1002 21:58:00.078000 3372986 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3374217 closing signal SIGTERM
Traceback (most recent call last):
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1245, in <module>
    main()
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1241, in main
    launch_command(args)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1220, in launch_command
    deepspeed_launcher(args)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/accelerate/commands/launch.py", line 906, in deepspeed_launcher
    distrib_run.run(args)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/lus/eagle/projects/SR-APPFL/duo/env/sft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3372986 got signal: 15
[default0]:[ds] Using CXX=/usr/bin/g++-12
[default2]:[ds] Using CXX=/usr/bin/g++-12
[default3]:[ds] Using CXX=/usr/bin/g++-12
[default1]:[ds] Using CXX=/usr/bin/g++-12
[default0]:[2025-10-02 21:53:51,081] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default2]:[2025-10-02 21:53:51,082] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default3]:[2025-10-02 21:53:51,080] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2025-10-02 21:53:51,080] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2025-10-02 21:53:58,055] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default2]:[2025-10-02 21:53:58,055] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default3]:[2025-10-02 21:53:58,060] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default1]:[2025-10-02 21:53:58,054] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[default0]:[2025-10-02 21:53:58,144] [INFO] [comm.py:821:init_distributed] cdb=None
[default2]:[2025-10-02 21:53:58,143] [INFO] [comm.py:821:init_distributed] cdb=None
[default3]:[2025-10-02 21:53:58,145] [INFO] [comm.py:821:init_distributed] cdb=None
[default1]:[2025-10-02 21:53:58,144] [INFO] [comm.py:821:init_distributed] cdb=None
[default0]:[2025-10-02 21:54:00,960] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[default3]:[2025-10-02 21:54:00,959] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[default1]:[2025-10-02 21:54:00,960] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
[default2]:[2025-10-02 21:54:00,959] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 8
